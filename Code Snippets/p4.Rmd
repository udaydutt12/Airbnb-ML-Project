---
title: "Project 4"
author: ' Kyunghoon Cha, Inkyoung Choi, Yeeuh Choi, and Uday Dutt'
date: "November 17, 2017"
output:
  html_document:
    code_folding: hide
    toc: yes
    toc_depth: 4
    toc_float: yes
runtime: shiny
resource_files:
- .Renviron
---
![](airbnb2.png)

```{r setup, include=FALSE}
library(ggplot2)
library (tree)
load('rdata.RData')
load('mydf.RData')
#srcFile <- 'clean_listings_summary1.csv'
#datin <- read.csv(srcFile, header=T)
#rdata<-datin[c(6:12,14:16)]
#rdata<-rdata[which(rdata$neighbPIN!='78733'),]
#rdata<-rdata[which(rdata$neighbPIN!='78717'),]# Remove data with PIN='78717'
#rdata<-rdata[which(rdata$neighbPIN!='78724'),]# Remove data with PIN='78724'
#rdata<-rdata[which(rdata$neighbPIN!='78726'),]# Remove data with PIN='78726'
#rdata<-rdata[which(rdata$neighbPIN!='78728'),]# Remove data with PIN='78728'
#rdata<-rdata[which(rdata$neighbPIN!='78730'),]# Remove data with PIN='78730'
#rdata<-rdata[which(rdata$neighbPIN!='78736'),]# Remove data with PIN='78736'
#rdata<-rdata[which(rdata$neighbPIN!='78738'),]# Remove data with PIN='78738'
############rdata<-rdata[which(rdata$neighbPIN!='78739'),]# Remove data with PIN='78739'
#rdata<-rdata[which(rdata$neighbPIN!='78754'),]# Remove data with PIN='78754'
#rdata$neighbPIN<-factor(rdata$neighbPIN) #changing to factor
#rdata<- na.omit(rdata)
#with(rdata,sum(is.na(revPerMonth)))
lprice<-log(rdata$price)
#rdata$lprice<-lprice
#mydf<-rdata[c(-5)] 
highPrice=ifelse (lprice <=5.03,"No"," Yes ")
#summary(mydf)
#head(mydf)
#str(mydf)
x<-mydf
```
#__AirBnB in Austin__




###__Introduction__
  This project will explore some data on AirBnB in Austin. Airbnb is an online marketplace and hospitality service, enabling people to lease or rent short-term lodging. It has over 3,000,000 lodging listings in 65,000 cities and 191 countries, and the cost of lodging is set by the host.Specifically, this project will cover the modeling of a dataset called "AirBnB in Austin" ([dataset](https://data.world/kurtakranz/s-17-dv-final-project/workspace/file?filename=clean_listings_summary.csv)) .
####__Summary of the Dataset:__


* neighbourhood
    + zip code of the location

* latitude
    + latitude of the location

* longitude
    + longitude of the location

* room type
    + room type of the location

* price
    + price that must be paid if you stay at the location

* minimum nights
    + minimum nights you must stay at the location

* number of reviews
    + total number of reviews the location has received

* number of reviews per month
    + number of reviews the location receives per month

* calcuated host listings count
    + the calculated value of the amount the host lists the location

* number of days available per year("availability")
    + number of days per year the location is available



 Specifically, this project will aim to predict price using Trees, SVM, and Unsupervised Learning Methods.

###__Data Presentation__

__Problem Statement__: We want to room predict price as a function other predictors existing in the dataset as we did in Project # 3.

__Data Preparation__: While going through the preliminary analysis following points are observed that needs to be kept in mind while preparing the data for analysis:

* Data set contains missing values thus the observations having missing values need to be removed.

* The response variable has non-linear relationships with several predictor variables (pl. see the scatter plot)

* The response variable doesn't have a bell-shaped distribution (pl. see box-whisker plots with various rmTypes and combined together.

* The neighbPIN ='78733' seems to be far away from the rest of the data thus it was removed considering it as outlier.

* While using the tree() function, the maximum number of allowed levels are 32. The existing data contains 41 levels. Thus, following neighbPIN were removed from the data set: 78717 (obs = 1), 78724 (obs = 9).,78726 (obs = 2), 78728 (obs = 3), 78730 (obs = 6), 78736 (obs = 5), 78738 (obs = 4), 78739 (obs = 6), and 78754 (obs = 9).

__Scatteplot Matrix of new Dataset:__
![](addplot1.png)



###__Trees__



####__Classification Tree__
```{r}
tree.fit =tree(highPrice~.-lprice ,mydf )
summary (tree.fit )
plot(tree.fit )
text(tree.fit ,pretty =0)
tree.fit
set.seed (1023)
train=sample (1: nrow(x ), 2450)
mydf.test=mydf [-train ,]
#str(mydf.test)
highPrice.test=highPrice[-train ]
#str(highPrice.test)
```

###### __Fitting the tree model using training data: __
```{r}
tree.fit =tree(highPrice~.-lprice ,mydf ,subset =train )
```

###### __Prediction using the fitted model and test data: __
```{r}
tree.pred=predict (tree.fit ,mydf.test ,type ="class")
table(tree.pred ,highPrice.test)
```
######__Results:__

* Cross Validation:
![](addplot2.png)

* Number of terminal nodes: 7

* Residual mean deviance: 0.9099 = 3428 / 3767

* Misclassification error rate: 0.2101 = 793 / 3774

* The Training Error rate is about 21%

* The residual deviance rate is about 91% indicating that the tree model does not give a good fit. 


#####__Pruning of Classification Tree__
```{r}
set.seed (3199)
prune.mydf =prune.misclass (tree.fit ,best =3)
plot(prune.mydf )
text(prune.mydf ,pretty =0)
tree.pred=predict (prune.mydf , mydf.test ,type="class")
table(tree.pred ,highPrice.test)
```
* Cross Validation:

![](addplot3.png)
####__Fitting A Regression Tree__

######__Define data that will be used for fitting Regression Tree:__
```{r}
regTree<-mydf[c(-11)]
str(regTree)
```
######__regTree data contains 3774 obs. of  10 variables__
```{r}
set.seed (1001)
train = sample (1: nrow(regTree ),2*nrow(regTree )/3)
```
######__training data set = 2516 obs, and testing = 1258 obs.__
```{r}
tree.regTree =tree(lprice~.,regTree ,subset =train)
summary (tree.regTree )
plot(tree.regTree )
text(tree.regTree ,pretty =0)
```

######__Determining Model improvement using pruning:__

* Cross Validation:
![](addplot4.png)
* Cross-Validation function cv.tree() function is used  to determine whether pruning the tree will improve performance: It indicates that the optimal single tree size is = 7 terminal nodes. 
```{r}
##********cv.regTree =cv.tree(tree.regTree )
#********plot(cv.regTree$size ,cv.regTree$dev ,type='b')
#Pruning the tree selected by cross-validation.
prune.regTree =prune.tree(tree.regTree ,best =7)
plot(prune.regTree )
text(prune.regTree ,pretty =0)
```

######__Predictions using unpruned tree:__
```{r}
yhat=predict (tree.regTree ,newdata =regTree [-train ,])
regTree.test=regTree [-train ,"lprice"]
plot(yhat ,regTree.test)
abline (0,1)
```

######__MSE:__

```{r}
mean((yhat -regTree.test)^2)
```

###__Boosting, Bagging and Random Forests__



####__Bagging__
```{r}
library (randomForest)
set.seed (1111)
bag.regTree =randomForest(lprice~.,data=regTree ,subset =train ,
                          mtry=9, importance =TRUE)
bag.regTree
```



###### __Performance of the bagged model:__
```{r}
yhat.bag = predict (bag.regTree ,newdata =regTree [-train ,])
plot(yhat.bag , regTree.test)
abline (0,1)
```

######__MSE:__

```{r}
mean(( yhat.bag -regTree.test)^2)
```


####__Random Forest__
```{r}
bag.regTree =randomForest(lprice~.,data=regTree ,subset =train ,
                          mtry=9, ntree =100)
yhat.bag = predict (bag.regTree ,newdata =regTree [-train ,])
```
######__MSE__
```{r}
mean(( yhat.bag -regTree.test)^2)
```

######__Use Smaller tree using mtry = p/3 around 3:__
```{r}
set.seed (1012)
rf.regTree =randomForest(lprice~.,data=regTree ,subset =train ,
                         mtry=3, importance =TRUE)
yhat.rf = predict (rf.regTree ,newdata =regTree [-train ,])
```
######__MSE__
```{r}
mean(( yhat.rf -regTree.test)^2)
```

######__Importance of each factor:__
```{r}
importance (rf.regTree )
varImpPlot (rf.regTree )
```

######__Performance of Random Forest:__
```{r}
rf.regTree=randomForest(lprice~.,data=regTree,subset=train)
rf.regTree
```
######__The MSE and % variance explained are based on OOB  or out-of-bag_ estimates, a very clever device in random forests to get honest error estimates. The model reports that $mtry=3$, which is the number of variables randomly chosen at each split. Since $p=9$ here, we could try all 9 possible values of mtry We will do so, record the results, and make a plot.__
```{r}
oob.err=double(9)
test.err=double(9)
for(mtry in 1:9){
  fit=randomForest(lprice~.,data=regTree,subset=train,mtry=mtry,ntree=400)
  oob.err[mtry]=fit$mse[400]
  pred=predict(fit,regTree[-train,])
  test.err[mtry]=with(regTree[-train,],mean((lprice-pred)^2))
  cat(mtry," ")
}
matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),
        type="b",ylab="Mean Squared Error")
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))
```

######__(Note: mtry=9`corresponds to bagging.)__

Based on the plot above, it appears that the best m-value is 2 or 3 at which MSE based on OOB and Test are equal and hence have no uncertainty. The random forest based model uses m = 3, and have the least MSE of 0.283 and it explains about 59.46% variation of the data.




####__Boosting__
```{r}
library (gbm)
set.seed (1231)
boost.regTree =gbm(lprice~.,data=regTree [train ,], 
                   distribution='gaussian', n.trees =5000, 
                   interaction.depth =7)
summary (boost.regTree )
```
######__Partial Dependence Plots__
```{r}
par(mfrow =c(1,3))
plot(boost.regTree ,i="rmType")
plot(boost.regTree ,i="revPerMonth")
plot(boost.regTree ,i="neighbPIN")
```

```{r}
yhat.boost=predict (boost.regTree ,newdata =regTree [-train ,],
                    n.trees =5000)
```

######__MSE:__
```{r}
mean(( yhat.boost -regTree.test)^2)
```
######__Using shrinkage factor of 0.01 rather default 0.001 and increasing n.tree to 10000__
```{r}
boost.regTree=gbm(lprice~.,data=regTree[train,],distribution="gaussian",
                  n.trees=10000,shrinkage=0.01,interaction.depth=7)
yhat.boost=predict (boost.regTree ,newdata =regTree [-train ,],
                    n.trees =10000)
mean(( yhat.boost -regTree.test)^2)
```

Based on the relative influence plot and the relative influence statistics output, one can see that rmType, revPerMonth, and neighbPIN are by far the most important variables.

The partial dependence plots for these three variables illustrate the marginal effect of the selected variables on the response after integrating out the other variables. These plots show that the median room price decreases rmType from Entire home/apt to other categories and number of revPerMonth while no definite pattern with respect to neighbPIN.

The test MSE obtained is 0.292; little inferior to the test MSE for random forests and superior to that for bagging.







######__Prediction on the test set. With boosting, the number of trees is a tuning parameter, computing the test error as a function of the number of trees, and make a plot.__
```{r}
n.trees=seq(from=100,to=10000,by=100)
predmat=predict(boost.regTree,newdata=regTree[-train,],n.trees=n.trees)
dim(predmat)
berr=with(regTree[-train,],apply( (predmat-lprice)^2,2,mean))
plot(n.trees,berr,pch=19,ylab="Mean Squared Error", 
     xlab="# Trees",main="Boosting Test Error")
abline(h=min(test.err),col="red")
```

###### __Using n.trees=2000 and interaction.depth=7:__
So far we have used the interaction.depth = 7 i.e., we are using 7 splits that may not be appropriate. In order to improve the boost model, one needs to find the optimal value of the interaction.depth parameter using cross-validation.
```{r}
boost.regTree=gbm(lprice~.,data=regTree[train,],distribution="gaussian",
                  n.trees=2000,shrinkage=0.01,interaction.depth=7)
yhat.boost=predict (boost.regTree ,newdata =regTree [-train ,],
                    n.trees =2000)
mean(( yhat.boost -regTree.test)^2)
```

######__Using n.tree to 2000 and varying interaction.depth__
```{r}
boost.regTree=gbm(lprice~.,data=regTree[train,],distribution="gaussian",
                  n.trees=2000,shrinkage=0.01,interaction.depth=6)
yhat.boost=predict (boost.regTree ,newdata =regTree [-train ,],
                    n.trees =2000)
mean(( yhat.boost -regTree.test)^2)
#
boost.regTree=gbm(lprice~.,data=regTree[train,],distribution="gaussian",
                  n.trees=2000,shrinkage=0.01,interaction.depth=5)
yhat.boost=predict (boost.regTree ,newdata =regTree [-train ,],
                    n.trees =2000)
mean(( yhat.boost -regTree.test)^2)
#
boost.regTree=gbm(lprice~.,data=regTree[train,],distribution="gaussian",
                  n.trees=2000,shrinkage=0.01,interaction.depth=4)
yhat.boost=predict (boost.regTree ,newdata =regTree [-train ,],
                    n.trees =2000)
mean(( yhat.boost -regTree.test)^2)
```

###__Support vector Machine__
####__Linear SVM__
######__Create binary variable from 'lprice' . Normalize the lat and long co-ordinates data:__
```{r}
mydf$hiprice=ifelse (mydf$lprice <=5.03,1,2)
mydf$nlat<-(mydf$lat-min(mydf$lat))/(max(mydf$lat)-min(mydf$lat))
mydf$nlong<-(mydf$long-min(mydf$long))/(max(mydf$long)-min(mydf$long))
head(mydf)
```

```{r}
x<-mydf[c(13,14)]
y<-mydf$hiprice
```
######__Plot the normalized coordinates and hiprice data__
```{r}
plot(x,col=y,pch=19)
legend("topright",legend=c("lprice <= 5.03","hiprice>5.03"),
       pch=19,col=c("red","black"))
#load the package `e1071` which contains the `svm` function
library(e1071)
dat=data.frame(x,y=as.factor(y))
str(dat)
```
######__Selecting Training & Testing Data Sets Randomly for SVM Classification. Training data set = 65% of 3774 = 2450. Test data set = 3774 - 2450 = 1324__
```{r}
set.seed (1023)
itrain=sample (1: nrow(mydf), 2450)
traindat =dat[c(itrain),]
testdat =dat[c(-itrain),]
str(traindat)
str(testdat)
svmfit=svm(y~.,data=traindat,kernel="linear",cost=0.01,scale=FALSE)
print(svmfit)
plot(svmfit,traindat)
```

######__Code for a better PLOT:__
```{r}
make.grid=function(x,n=100){
  grange=apply(x,2,range)
  nlong=seq(from=grange[1,1],to=grange[2,1],length=n)
  nlat=seq(from=grange[1,2],to=grange[2,2],length=n)
  expand.grid(nlong=nlong,nlat=nlat)
}
xgrid=make.grid(x)
ygrid=predict(svmfit,xgrid)
plot(xgrid,col=c("red","blue")[as.numeric(ygrid)],pch=20,cex=.2)
points(x,col=y,pch=19)
points(x[svmfit$index,],pch=5,cex=2)
```

######__Identifying observations that are Support Vectors:__
```{r}
svmfit$index
```
######__Basic information about the support vector classifier:__
```{r}
summary (svmfit)
```
######__What happens if a larger value of the cost parameter used?__
```{r}
svmfit=svm(y~.,data=dat,kernel="linear",cost=1,scale=FALSE)
plot(svmfit , dat)
svmfit$index
summary (svmfit )
```
######__Tuning SVMs with a linear kernel, using a range of values of the cost parameter.__
```{r}
set.seed (101)
tune.out=tune(svm,y~.,data=dat ,kernel ="linear",
              ranges =list(cost=c(0.001 , 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
```
######__Best model parameters based on CV (automatically stored into tune function)__
```{r}
bestmod =tune.out$best.model
summary (bestmod)
```
######__Predict the class label on a set oftest observations__
```{r}
ypred = predict(bestmod,testdat)
table(predict =ypred,truth=testdat$y)
```
####__Nonlinear SVM__
```{r}
svmfit =svm(y~.,data=dat [itrain ,], kernel ="radial", 
            gamma =1,cost =1)
plot(svmfit,dat[itrain ,])
```

######__Information about the SVM fit__
```{r}
summary (svmfit)
```
######__Increase the value of cost to reduce the number of training errors__
```{r}
svmfit=svm(y~.,data=dat[itrain ,], kernel="radial",gamma=1,cost=100)
plot(svmfit ,dat [itrain ,])
```


######__Perform cross-validation using tune() to select the best choice of variables and cost for an SVM with a radial kernel__


```{r}
set.seed (1)
tune.out=tune(svm, y~.,data=dat[itrain ,],kernel ="radial",
              ranges =list(cost=c(0.1 ,1 ,10 ,100 ,1000),
                           gamma=c(0.5,1,2,3,4)))
summary (tune.out)
```
######__Making Predictions__
```{r}
table(true=dat[-itrain ,"y"], pred=predict (tune.out$best.model ,
                                            newdata =dat[-itrain,]))
```


![](addplot5.png)


![](addplot6.png)
###__PCA, K Means Clustering, and Hierachical Clustering__

####__Principal COmponent Analysis__

The objective of PCA is the understand how to group the multi-variate data into groups. Further, understanding whether it is possible to transform correlated variables into a smaller number of uncorrelated variables.

###### __Create new data for PCA by removing categorial variables__
```{r}
pcadat<-mydf[c(5:10)]
head(pcadat)
apply(pcadat,2,mean) # Determine column means
apply(pcadat,2,var) # Determine column stdev
# Standardize/normalize the data 
pcadat <- scale(pcadat)
head(pcadat)
pca.out=prcomp(pcadat, scale=TRUE)
pca.out
names(pca.out)
```
######__Plot the first two principal components__
```{r}
biplot(pca.out, scale=0)
```
######__Principal component loading vectors (Eigen Vectors)__
```{r}
pca.out$rotation
```
######__Standard deviation of each principal component__
```{r}
pca.out$sdev
```
######__Variance explained by each principal component__
```{r}
pca.var <- pca.out$sdev ^2
pca.var
```
######__Proportion of variance explained by each principal component__
```{r}
pve=pca.var/sum(pca.var )
pve
```
######__Plot the PVE explained by each component, as well as the cumulative PVE, as follows:__
```{r}
plot(pve , xlab="Principal Component", ylab =" Proportion of Variance Explained ", 
     ylim=c(0,1) ,type='b')
plot(cumsum (pve ), xlab=" Principal Component ", ylab ="
     Cumulative Proportion of Variance Explained ", ylim=c(0,1) ,
     type='b')
summary(pca.out)
```

####__K-Means Clustering__
```{r, include=FALSE}
# Create new data for KMEAN by removing categorial variables
kmdat<-mydf[c(5:10)]
head(kmdat)
#Remove missing observations from the data
na.omit(kmdat)
```

```{r}
# Standardize numeric variables
skmdat <- scale(kmdat)
# check that we get mean of 0 and sd of 1
apply(skmdat, 2, mean)
apply(skmdat, 2, sd)
set.seed(101)
km.out<-kmeans(skmdat,3,nstart=50)
#km.out
km.out$size
km.out$cluster
```
######__Evaluation with respect to rmType__
```{r}
table(mydf$rmType,km.out$cluster)
plot(mydf[c("revPerMonth","lprice")], col=km.out$cluster)
plot(mydf[c("revPerMonth","lprice")], col=mydf$rmType)
```
######__Evaluation with respect to Price__
```{r}
set.seed(10101)
km.out<-kmeans(skmdat,2,nstart=50)
table(mydf$highPrice,km.out$cluster)
plot(mydf[c("revPerMonth","lprice")], col=km.out$cluster)
plot(mydf[c("revPerMonth","lprice")], col=mydf$highPrice)
```

####__Hierarchical Clustering__


######__Create new data for KMEAN by removing categorial variables__
```{r}
clust.dat<-mydf[c(6:7)]
#head(clust.dat)
#Remove missing observations from the data
#na.omit(clust.dat)
```
######__Use Complete cluster method__
```{r}
clust.complete=hclust(dist(clust.dat),method="complete")
plot(clust.complete)
```
######__Cut off the tree at the desired number of clusters using cutree__
```{r}
clusterCut <- cutree(clust.complete, 3)
table(clusterCut, mydf$rmType)
```
######__Try whether one can improve using a different linkage method__
```{r}
clust.average=hclust(dist(clust.dat),method="average")
plot(clust.average)
```
######__Try whether one can improve using a different linkage method__
```{r}
clust.single=hclust(dist(clust.dat),method="single")
plot(clust.single)
```
# Plot Identified Clusters
```{r}
ggplot(mydf, aes(numReviews, revPerMonth, color = mydf$rmType)) + 
  geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) + 
  scale_color_manual(values = c('black', 'red', 'green'))

ggplot(mydf, aes(numReviews, lprice, color = mydf$rmType)) + 
  geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) + 
  scale_color_manual(values = c('black', 'red', 'green'))

ggplot(mydf, aes(revPerMonth, lprice, color = mydf$rmType)) + 
  geom_point(alpha = 0.4, size = 3.5) + geom_point(col = clusterCut) + 
  scale_color_manual(values = c('black', 'red', 'green'))
######################END OF CODE#########################################
```
##__Conclusion__
Overall, comparing project 4 to project 3 for our dataset is a very bad comparison. This is not because our models that were derived from project 3 were much better than the models derived in project 4. Instead, it is because these two projects use two different datasets that were manipulated and cut so that certain models would work. For example, in order to use trees, we cannot use a explanatory variable with over 31 factors. This dataset included 42 different Austin area zip codes. Thus, we cut off about 9 zip codes of data from the dataset in order to do project 4.

Another flaw in comparing project 3 to project 4 is that both projects are based on a bad set of data.The data are not very correlated with one another, so no matter what we do the results won;t magically become super good.

We can still compare the models based on errors, though. Compared to the methods from project 3, we have seen the following from the models in project 4:

* In the Classification Tree, the residual deviance rate is about 91% indicating that the tree model does not give a good fit. This is worse than project 3.

* After pruning, we were able to simplify the classification tree from 7 nodes to 3 nodes without losing any efficiency in the model

* From the regression tree model, we found that Sum of Squared Errors=0.326

* After doing bagging on the Regression tree, we get MSE=.295, which is a little bit better

* After using random forest, we get MSE=0.283, which is even better 

* After boosting the Regression Tree, we get MSE=0.279, which is the best one. __Thus, we selected boosting to be the best model for our data.__

* SVM analysis was bad. Fitting a line is not suitable for our dataset, as there is just too much data. If there were less data, then SVM might have done a better job. 

* Nonlinear SVM was conducted by setting kernel='radial', and this gave us a better model.

* PCA and K means clustering were conducted. In general, they were ineffective. There were too many data points for effective clustering. 
